\section{A supervised model of dyadic sentiment}

\label{section:foreign_relations_supervised_model}

In the last chapter we described a model for identifying influential
documents.  An important feature of that model was that it was
unsupervised; only after it was fit did we compare the inferred
influence of an article with the number of citations it had recieved.
In this section we will take a more direct approach, fitting a
model with labels \emph{defined} to represent sentiment.

Here will make further assumptions about the object of discussion in
the text. We will assume in particular that each country can be
described by a vector in some latent space.  The relationship between
two countries is then determined (up to stochasticity) by the
relationship between these countries' positions in this latent space
(models that make this assumption are sometimes referred to as latent
space models or spatial models).  A spatial a model provides two
benefits for us. First, it provides interpretability: nations with similar
positions in this latent space tend to interact more positively, while
nations further apart tend to have more tension in their relationship.
A spatial model also allows us to draw on existing work from
multidimensional scaling, which has been used successfully in both
political science \cite{martin:2002,jackman:2001} and social network
modeling \cite{hoff:2002,chang:2009}.

\begin{figure}
  \center
  \vspace{-55pt}
  \includegraphics[width=0.5\textwidth]{chapter_foreign_relations/figures/countries_gm.pdf}
  \caption{A time-series model of countries' interactions.
    Pseudo-observations of ``zero'' are added for regularization.
    Amazon Mechanical Turk labels are used to fit $\beta$, which is
    used to infer unobserved sentiments.}
  \label{figure:gm}
\end{figure}

\subsection{A temporal model of interaction.}
We formalize the latent space assumption by letting each country $c$
take a position $\bar x_c \in \mathbb{R}^P$ in a space of latent
political sentiment. The relationship between two countries $c_1, c_2$
will be described by a scalar $s_{c_1,c_2} \in \mathbb{R}$.  This
sentiment is determined by the interaction of their positions:
$s_{c_1, c_2} = \mathcal{F}(\bar x_{c_1}, \bar x_{c_2})$, for some
suitable function $\mathcal{F}$.

Foreign relations are not static; nations' alliances and preferences
change over time with the evolution of economies, technology, and
culture.  Therefore we make this a fully temporal model by
allowing each country's mean position $\bar x_{c,t}$ to drift over
time with the Markov transition
\begin{align}
  \bar x_{c,t} | \bar x_{c,t-1} \sim \mathcal{N}(\bar x_{c,t-1}, \sigma_{\mbox{\tiny chain}}^2),
\end{align}
as shown in Figure~\ref{figure:gm}. At any time $t$, state $c_1$ may interact with state $c_2$ in the following way:
\begin{align}
  x_{c_1,d} \sim \mathcal{N}(\bar x_{c_1, t}, \sigma_D^2) \nonumber \\
  x_{c_2,d} \sim \mathcal{N}(\bar x_{c_2, t}, \sigma_D^2) \nonumber \\
  s_d := x_{c_1,d}^T x_{c_2,d}, \label{eq:sentiment_space}
\end{align}
where we interpret $s_d$ as the sentiment between $c_1$ and $c_2$ as
reflected by article $d$.  When $c_1$ and $c_2$ are similar (as
measured by their inner product), their sentiment $s_d$ will be
positive; if they are dissimilar, their sentiment will be negative.
More extreme values indicate stronger sentiment.

\subsection{Inferring sentiment from text.}
\label{section:text_regression}
When a news source discusses the relationship between these nations,
the author's choice of words $\bm w_d$ reflects the relationship
between the countries.  We model this sentiment with the text of the
article $d$.  Using text regression \cite{kogan:2009}, we model
sentiment using the wordcounts $\bm w_d$ of the article:
\begin{align}
  s_d | \bm w_d, \beta \sim \mathcal{N}( \bm w_d^T \beta, \sigma_W^2 ).
  \label{eq:sentiment_text}
\end{align}
For the purposes of countries' positions in the latent-space model, we
will assume that $\beta$ is observed (we describe how to fit $\beta$
with \myeq{sentiment_text} and human labels in
Section~\ref{section:mturk}).  In fitting $\beta$, we interpret the
distribution of $s_d$ as a Gaussian with a mean which depends on the
text.

For the purposes of countries' positions in the latent-space model,
however, we will use the symmetry of the Gaussian to model text as if
it were conditioned on sentiment: $\mathcal{N}(s_d | \bm w_d^T \beta,
\sigma_w^2) = \mathcal{N}( \bm w_d^T \beta | s_d, \sigma_W^2 )$.  This
allows us to reconcile \myeq{sentiment_text} and
\myeq{sentiment_space}, so that the distribution of sentiment
conditioned on text and countries' positions is:
\begin{align}
  p(s_d | \bm w_d, \beta, x_{c_1,d}, x_{c_2,d}) & \propto
  \mathcal{N}(\bm w_d^T \beta | s_d, \sigma_W^2 )
  \mathcal{N}(x_{c_1,d} | \bar x_{c_1, t}, \sigma_D^2)
  \mathcal{N}(x_{c_2,d} | \bar x_{c_2, t}, \sigma_D^2) \\
  & \hspace{20pt} \mbox{ such that } s_d = \mathcal{F}(x_{c_1,t},
  x_{c_2,t})
  \\
  & = 
  \mathcal{N}(\bm w_d^T \beta |
    \mathcal{F}(x_{c_1,t}, x_{c_2,t}), \sigma_W^2 )
  \mathcal{N}(x_{c_1,d} | \bar x_{c_1, t}, \sigma_D^2)
  \mathcal{N}(x_{c_2,d} | \bar x_{c_2, t}, \sigma_D^2).
\end{align}

% In addition, a UN resolution may come up for vote at any time.  States
% cast a vote based on their current positions:
% \begin{align}
% x_{c_1,d} \sim N(\bar x_{c_1, t}, \sigma_D^2) \nonumber \\
% p(v_{cr}) = \sigma(x_{c_2, t} b_r + a_r) \nonumber \\
% \end{align}

% \begin{wrapfigure}{r}{0.4\textwidth}
%   \includegraphics[width=0.4\textwidth]{figs/countries_gm.pdf}
%   \caption{The full time-series model of interaction by countries.
%     The large plate shows replication of a Markov chain for each
%     country.  Certain countries interact at each epoch -- possibly
%     multiple times -- with sentiment $s$.}
%   \label{fig:countries_by_ip}
% \end{wrapfigure}

\paragraph{A brief comment on notation.} Before proceeding to
inference and the experimental validation of this model, we pause to
summarize our use of notation.  In this chapter, we will use notation
flexibly when it is convenient.  The typical unit of discussion will
be the $d$th document occurring at time $t$.  The $d$th document
discusses two countries, $c_1$ and $c_2$; these define a tuple $(c_1,
c_2, d, t) = (c_2, c_1, d, t)$.  We will generally use $d$ to index
documents, $t$ or $s$ to index time, and $c$ to index a country.  When
document $d$ is given, we may refer to its time $t_d$ (which is
unique) or to the two interacting countries as $c_{d,1},c_{d,2}$ or
$c_1,c_2$.  Alternatively, we may refer to the documents in which a
country $c$ appears as $d_{c,1}, \ldots, d_{c,1}$.
For example, we may index the variable $x_{(c_1,d,t)}$ variously as
$x_{c_{d,1}}$ or $x_{d,1}$, or even $x_c$ if the context is clear. The
sentiment between two countries might be described as $s_d$,
$s_{c_1,c_2}$, $s_{d,t}$, or $s_{c_1,c_2,d,t}$.

\subsection{Related work}

% Democracy, Political Similarity, and International Alliances, 1816-1992
% Brian Lai
% Dan Reiterb
% Department of Political Science, Emory University
Spatial models such as Item Response Theory (IRT) have been developed
over the past century by quantitative social scientists for analyzing
behavior.  While much of this work has been used to model
parliamentary voting behavior, these techniques have also been used to
model voting in the UN General Assembly. Gartzke et al., for example,
use these votes and alliance models to study the nations' affinities
\cite{gartzke:1998}.

These models have been developed for dyadic data more fully in network
models such as the latent space model \cite{hoff:2002,sarkar:2005}, in
which the probability of a link between two nodes is a function of
their latent-space distance.  The qualitative relationship of
entities' dyadic relationships has been more fully developed with text
by the relational topic model, which uses free text to model the
relationship between actors in an unsupervised setting
\cite{chang:2009}.
% Supervised topic models


% Affinity of states dataset: 
% Fading Friendships (working paper)
% Alliances, Affinities and the Activation of International Identities∗
% Erik Gartzke†
% Alex Weisiger‡
% 7 March 2011

\subsection{Inference}
We fit the \emph{MAP} objective of this probabilistic model.  This has
the benefit of both clean exposition and simple implementation, and it
can be interpreted as a form of unregularized variational inference.
We optimize the \emph{MAP} objective in this model using an
expectation maximization (EM) algorithm.

We designed the probabilistic objective to make inference tractable.
For example, the Gaussian distribution of per-document sentiment
variables $x_{c, t} | \bar x_{c,t}$ makes inference for $\bar x_{c,t}$
closed-form.  Countries' per-interaction positions $x_{c_{d,1}, t},
x_{c_{d,2},t} | \bar x_{c_{d,1},t}, \bar x_{c_{d,2}, t}, s_{c_{d1},
  c_{d2}}$ are then fit using gradient ascent.

\subsubsection{M Step.} In the M step, we estimate the mean $\bar
x_{c,t} | x_{c_1,1}, \ldots, x_{c_N,T}$ of each country's position
using a modified Kalman filter (modified because it must take into
account all interactions in a given year).  This step differs from a
standard Kalman filter in that we may have no or multiple observations
on any given date.\footnote{We also experimented with
  \emph{pseudo-observations} for each country at each day $t$ with
  mean 0 and variance $\sigma_p^2$.  These observations are a form of
  ``time-series regularization'' and reflect the sense that a lack of
  news is effectively neutral news.  Low values of $\sigma_p^2$ harmed
  performance, and it was best set around $10^6$.}  The prior over the
ends of the chain are standard normal.

\paragraph{Kalman updates.}
The M step seeks the most-likely mean position $\bar x_c$ for each
country $c$ given its $D_{c,s}$ interactions at each time $s=1, \ldots,
T$
\begin{align}
  \arg \max \bar x_{c,t} | x_{c,1,1}, \ldots, x_{c,D_{c,1},1}, x_{c,D_{c,2},2},
  \ldots, x_{c,D_{c,t},T},
\end{align}
for $t=1, \ldots, T$. The optimal value for $\bar x$ can be found with
a modified Kalman smoother \cite{kalman:1960}.  This modified Kalman
smoother requires a forward filter step and a backward filter step.
The forward filter estimates the mean position given all previous
observations (note that we are liberal with notation and use $x_c,d,t$
to describe the position of country $c$ at time $t$ for interaction
$d$:
\begin{align}
  \bar x_{\mbox{\tiny forward},c,t} | \bar x_{\mbox{\tiny forward},c,t-1}, x_{c,1,t-1}, \ldots, x_{c,D_{c,t-1},t-1}
  & \gets \frac{\bar x_{\mbox{\tiny forward},c,t-1} / \sigma_{\mbox{\tiny forward},t-1}^2
    + \sum_{d=1}^{D_{c,t-1}} x_{c,d,t-1} / \sigma_{\mbox{\tiny obs}}^2}
  {1 / \sigma_{\mbox{\tiny forward},t-1}^2 + 1 / \sigma_{\mbox{\tiny obs}}^2} \\
  \sigma_{\mbox{\tiny forward},t}^2
  & \gets \frac{1}{1 / \sigma_{\mbox{\tiny forward},t - 1}^2
    + D_{c,t-1} / \sigma_{\mbox{\tiny obs}}^2} + \sigma_{\mbox{\tiny chain}}^2,
\end{align}
with initial condition $\bar x_{c,0} = 0,
\sigma_{\mbox{\tiny forward},0}^2=10$.  The backward step estimates
the chain's mean given all current and future observations:
\begin{align}
  \bar x_{\mbox{\tiny backward},c,t} | \bar x_{\mbox{\tiny backward,c,t+1}}, x_{c,1}, \ldots, x_{c,D_{c,t}}
  & \gets \frac{\bar x_{\mbox{\tiny backward},c,t+1} / \sigma_{t+1}^2
    + \sum_{d=1}^{D_{c,t}} x_{c,d,t} / \sigma_{\mbox{\tiny obs}}^2}
  {1 / \sigma_{\mbox{\tiny backward},t-1}^2 + 1 / \sigma_{\mbox{\tiny obs}}^2} \\
  \sigma_{\mbox{\tiny backward},t}^2
  & \gets \frac{1}{1 / (\sigma_{\mbox{\tiny backward},t + 1}^2 + \sigma_{\mbox{\tiny chain}}^2)
    + D_{c,t} / \sigma_{\tiny obs}^2},
\end{align}
with initial conditions $\bar x_{\mbox{\tiny backward},c,T} = 0, \sigma_{\mbox{\tiny
    backward},T}^2=10$. The smoothed means given all observations are then:
\begin{align}
  \bar x_{c,t} | \bar x_{\mbox{\tiny forward,c,t}}, \bar x_{\mbox{\tiny backward,c,t}}
  & \gets \frac{\bar x_{\mbox{\tiny forward},c,t} / \sigma_{\mbox{\tiny forward},t}^2
    + \bar x_{\mbox{\tiny backward},c,t} / \sigma_{\mbox{\tiny backward},t}^2}
  {1 / \sigma_{\mbox{\tiny forward},t}^2
    + 1 / \sigma_{\mbox{\tiny backward},t}^2}
\end{align}

% x_{c,d,t}
\subsubsection{E Step.} In the E-step, our goal is to infer each
nation's position $x_{c_{d,1}} | \bar{x}_{c,d,t}, x_{c_{d,2}}, s_d$
during interaction $d$ given its mean $\bar x_{c_{d,1},t_d}$ and text
$\bm w_d$ of describing this interaction, \emph{and} given the other
country's position for this interaction.  We find these positions by
gradient ascent on each interaction:
\begin{align}
  & {\arg \max}_{x_{c_{d,1},t}, x_{c_{d,2},t}}
  p(\bm w_d^T \beta | \mathcal{F}(x_{c_{d,1},t}, x_{c_{d,2},t}),
  \bar x_{c_{d,1},t}, \bar x_{c_{d,2},t}) \\
  = & 
  {\arg \max}_{x_{c_{d,1},t}, x_{c_{d,2},t}}
  p(\bm w_d^T \beta | \mathcal{F}(x_{c_{d,1},t}, x_{c_{d,2},t}), \beta)
  p(x_{c_{d,1},t} | \bar x_{c_{d,1},t})
  p(x_{c_{d,2},t} | \bar x_{c_{d,2},t}).
\end{align}

\subsection{Empirical studies: comparisons with ground truth}
We now turn to an experimental analysis of this model.  We first
describe the human labels we have used to define sentiment $s_d$
within this model.  We then describe two news archives from which we
have inferred countries' sentiment over time. text in which the
relationships of countries are deto which we applied this model and .
We then turn

\subsection{Sentiment labels $s$}
\label{section:sentiment_models}

To infer the sentiment $s_d$ between two countries, we treat the
corresponding news article as a bag of words and fit text regression
\cite{kogan:2009} to predict the sentiment from the text.  We used two
sources for sentiment labels: Amazon Mechanical Turk and Covariates of
War.  In each case, we fit a text regression which predicted the
sentiment $s_d$ from a training set of text snippets.

\subsubsection{Human labels with Amazon Mechanical Turk}
\label{section:mturk}

\emph{Amazon Mechanical Turk} (AMT) is a crowdsourcing platform which
provides access to thousands of ``workers'' who perform simple tasks
over the Internet.  Although the ``requestor'' can use tests to
demonstrate that , these workers are typically not experts.

\begin{figure*}
  \setlength\fboxsep{0pt}
  \setlength\fboxrule{0.5pt}
  \center \fbox{\includegraphics[width=1.0\textwidth]{chapter_foreign_relations/figures/mturk_screenshot.png}}
  \label{figure:mechanical_turk_sample}
  \small\caption{A screenshot of a Mechanical Turk labeling task.
    Sometimes relationships may be complicated; both raters gave this
    example a score of ``slightly positive''.}
  \normalsize
\end{figure*}

To fit the model, we asked \emph{Amazon Mechanical Turk} raters to rate the
sentiment between two nations mentioned in a paragraph in the range -5
= "mortal enemies'', $\ldots, 5 = $''very good relationship''.
With these training examples, we fit the coefficients $\beta$ of the
text regression discussed in Section~\ref{section:model}.  This
coefficient was then fixed in the joint model in
Figure~\ref{figure:gm} to allow us to learn sentiment from
paragraphs' words.  We trained and fit the model with the following procedure:

\begin{enumerate}
  \item Randomly select 3607 paragraphs discussing pairs of 245 countries.
  \item Label each of these paragraphs' sentiment with two ratings
    from Amazon Mechanical Turk.
  \item Hold out 42 random country pairs (244 paragraphs) for testing.
  \item Estimate sentiment model parameters $\beta$ using the training
    paragraphs.
  \item Infer the spatial sentiment model with these parameters on
    all 257,472 paragraphs.
  \item Evaluate model sentiment prediction on the heldout 244 test
    paragraphs.
\end{enumerate}

\subsubsection{Expert labels: Correlates of War}
\label{section:correlates_of_war}

We also used a set of expert labels called the \emph{Covariates of
  War} (CoW).  These labels describe the relationships between
countries created over the past

\subsubsection{Datasets and tokenization}  We made an important
assumption that the scope of foreign sentiment discussion is at the
level of a paragraph.  We therefore used the subset of paragraphs
which discuss exactly two nations as ``documents'' $d$, a set of
257,472 distinct paragraphs which came from the foreign, business,
financial, and magazine desks of the newspaper during this period. We
used standard stopword filters and removed the geographic tags for
labeling nations from the sentiment vocabulary.

\label{section:experiments}

\paragraph{New York Times.} We fit and evaluate this model over news articles discussing 245
nations from twenty years of the \emph{New York Times} (NYT).  This
collection spanned the years 1987 to 2007, a period which included
both Gulf wars; the collapse of the Soviet Union; the reunification of
Germany; September 11th, 2001; and countless other world events.

\paragraph{...}
TODO(sgerrish): Add another dataset.
  
\subsubsection{Results}
 \begin{figure}
  \begin{tabular}{ccc}
    \includegraphics[width=0.33\textwidth]{chapter_foreign_relations/figures/002_countries_by_ip_1987.pdf} &
    \includegraphics[width=0.33\textwidth]{chapter_foreign_relations/figures/002_countries_by_ip_2007.pdf} &
    \includegraphics[width=0.33\textwidth]{chapter_foreign_relations/figures/002_us_vs_everyone.pdf} \\
    (a) & (b) & (c) \\
  \end{tabular}
  \caption{
    (a) Example positions of selected countries in the latent space of national sentiment, in 1987. Sentiment is given by the inner product between two vectors.
    (b) Example positions of selected countries in 2007.
    (c) Mutual sentiment $s = \bar x_{c, \cdot}^T \bar x_{\mbox{\small us}, \cdot}$ with the United States over time. The two Iraq wars and September 11th, 2001 are marked.
  }
  \label{figure:figures}
\end{figure}

\paragraph{Heldout sentiment}
 \begin{table}[t]
   \caption{Average error predicting sentiment $s_d$ between heldout nation pairs.}
   \begin{center}
   \begin{tabular}{|c|c|c|}
     \hline
     Model & Mean Squared Error & Mean Absolute Error \\
     \hline
     Inter-rater agreement & 1.77 (7.11) & 1.037 (2.07) \\
     \hline
     Text regression & 5.53 & 1.94 \\
     \hline
     Prior variance 0.1 & 2.36 & 1.09 \\
     Prior variance 1 & \textbf{2.32} & \textbf{1.07} \\
     Prior variance 10 & 2.32 & 1.08 \\
     Prior variance 100 & 2.34 & 1.09 \\
     Prior variance 1000 & 2.33 & 1.08 \\
     \hline
   \end{tabular}
   \label{figure:sse_test}
   \end{center}
\end{table}


\paragraph{Heldout sentiment}
 \begin{table}[t]
   \caption{Average error predicting sentiment $s_d$ between heldout nation pairs.}
   \begin{center}
   \begin{tabular}{|c|c|c|}
     \hline
     Model & Mean Squared Error & Mean Absolute Error \\
     \hline
     Inter-rater agreement & 1.77 (7.11) & 1.037 (2.07) \\
     \hline
     Text regression & 5.53 & 1.94 \\
     \hline
     1 & \textbf{2.33} & \textbf{1.08} \\
     2 & 2.37 & 1.09 \\
     3 & 2.36 & 1.09 \\
     4 & 2.38 & 1.09 \\
     5 & 2.40 & 1.10 \\
     6 & 2.38 & 1.10 \\
     7 & 2.40 & 1.10 \\
     \hline
   \end{tabular}
   \label{figure:sse_test}
   \end{center}
\end{table}

 \begin{table}[t]
   \caption{Average error predicting sentiment $s_d$ between heldout nation pairs for Correlates of War.}
   \begin{center}
   \begin{tabular}{|c|c|c|c|}
     \hline
     Dimension & & Mean Squared Error & Mean Absolute Error \\
     \hline
     Text regression & & 3.37 & 1.12 \\
     \hline
     Mean & & 3.68 & 1.19 \\
     \hline
     \hline
     Dimension & Reversion variance & Mean Squared Error & Mean Absolute Error \\
     \hline
     1 & 10 & 0.628 & 0.389 \\
     2 & 10 & 0.561 & 0.287 \\
     \textbf{3} & 10 & 0.559 & 0.286 \\
     4 & 10 & 0.562 & 0.308 \\
     5 & 10 & 0.557 & 0.306 \\
     6 & 10 & 0.535 & 0.142 \\
     7 & 10 & 0.535 & 0.140 \\
     \hline
     3 & 0.1 & 0.539 & 0.163 \\
     3 & 1 & 0.555 & 0.245 \\
     3 & \textbf{10} & 0.559 & 0.286 \\
     3 & 100 & 0.560 & 0.296 \\
     3 & 1000 & 0.561 & 0.298 \\
     3 & 10000 & 0.561 & 0.298 \\
     \hline
   \end{tabular}
   \label{figure:sse_test}
   \end{center}
\end{table}

For two nations $c_1$ and $c_2$ mentioned together at time $t$, we
predict their sentiment to be $\tilde s_{c_1, c_2} = \bar x_{c_1,t}
\bar x_{c_2, t}$.  Based on this estimate, the average squared error
for heldout nations is 2.32 ($R^2=0.51$), considerably better than a
baseline of text regression, which had means-squared-error $5.53$; for
comparison, average \emph{inter-rater squared error} -- the minimum
theoretically possible given the set of Mechanical Turk ratings -- was
$1.77$, and the square of the difference \emph{between} raters was
$7.11$.  We also found that using ``pseudo observations'' with even
modest observation variance improves predictive performance; these
results are summarized in Table~\ref{figure:sse_test}.

Nations' positions are illustrated in Figure~\ref{figure:figures}.
Figures~\ref{figure:figures}(a,b) show nations' relative positions
and interactions for a two- dimensional model; heldout error slightly
increased for higher dimensions. The United States' relationship with
Iraq (see Figure~\ref{figure:figures}(c)) serves as an excellent
example of this model in action; the relationship between these
nations degrades during both the Gulf War and the invasion of Iraq
following September 9th, 2001.  Israel's relationship with the United
States demonstrates one of the model's downfalls: while Israel is
considered a close ally of the United States, the raters' 95 ratings
of these nations' mentions had mean 0.12 and standard deviation
2.65.  Because the model is only as good as its ratings, the nations
appear to have a rocky relationship.

% > a = read.csv("../../data/v4/v4-doc-training_samples.csv", as.is=TRUE, header=FALSE)


  %  We then evaluate perplexity
 % \begin{align}
 %   \mbox{perp}_d & = \mathcal{E_{\hat D}} \log p(w | \bar x_{d_{c1}}
 %     \bar x_{d_{c2}} ) \\
 %     & = \frac{1}{N} \sum_N \sum_{W_{d_n}}
 %     \log p(w | \bar x_{d_{c1}} \bar x_{d_{c2}} ) \\
 % \end{align}

% \subsubsection*{Bivariate change point detection}
% In addition to identifying the latent positions of nations over
% time, we can pinpoint periods of great change or upheaval.  Sudden
% changes in a nation's position is an indication of newsworthy events
% in its history; simultaneous changes in two nations' positions is an
% indication that they are both taking part in these newsworthy events.

% The task of identifying sudden changes in a time-series is known as
% change point detection.  Change point detection is frequently
% addressed by simple univariate significance tests.  We consider
% changes in the statistic $\bar s_{c1,c2} := \bar x_{c1} \bar x_{c2}$
% (see Equation~\ref{figure:sentiment}).  Because $\bar s_{c1,c1}$ may
% be change significantly when either $x_{c_1}$ or $x_{c_2}$ changes, we
% search for simultaneous significant changes in $\bar x_{c_1}$, $\bar
% x_{c_2}$, and $\bar s_{c1,c2}$ (Note that $\mathcal{E}[s_{c1,c2}] \neq
% \bar s_{c1,c2}$; we compute $\bar s_{c1,c2}$ out of convenience.

