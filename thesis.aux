\relax 
\@writefile{toc}{\contentsline {section}{Abstract}{iii}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{iv}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Quantitative methods for social research in the digital age}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{lazer:2009}
\citation{national_archives:2012a,national_archives:2012b}
\citation{googleblog:2008}
\citation{pearl:1985}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Organization}{5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminary material: quantitative methods}{7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chapter:introductory_material}{{2}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Standards and naming conventions}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Notation used throughout this thesis.}}{8}}
\newlabel{section:pipeline}{{2.1}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Latent-variable models for prediction and exploratoration}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Data analysis pipepline}{8}}
\newlabel{section:data_analysis_pipeline}{{2.2.1}{8}}
\citation{box:1980,gelman:1996}
\citation{tufte:2001}
\citation{wilkinson:2005}
\citation{chaney:2012}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A data analysis pipeline. In this work, we discuss elements of defining modeling assumptions, model implementation, and model revision. We will focus on applications which use text data.}}{10}}
\newlabel{figure:data_analysis_pipeline}{{2.1}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Latent-variable models}{10}}
\newlabel{lvm:matching}{{2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Undirected graphical models for summarizing distributional assumptions}{11}}
\citation{bishop:2006}
\citation{bishop:2006}
\citation{grimmer:submitted}
\citation{blei:2003}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Left: graphical model for a unigram language model. Documents $1, \ldots  , D$ are treated as \emph  {bags of words}, or collections of words $w_n$. Right: graphical model for Latent Dirichlet Allocation. Circles are random variables, arrows connote dependency, and plates represent replication. The circles represent observed random variables (words in this case).}}{13}}
\newlabel{fig:bagofwords_lda_gm}{{2.2}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Text as a medium for social science analysis}{13}}
\newlabel{section:text_intro}{{2.2.4}{13}}
\@writefile{toc}{\contentsline {subsubsection}{Latent Dirichlet Allocation}{14}}
\citation{salakhutdinov:2008a}
\citation{clinton:2004,martin:2002,poole:1991,enelow:1984,albert:1992}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Example topics from Latent Dirichlet Allocation fit to sentences from the the textbook \emph  {Biology} by Campbell and Reece. This is a small subset of the 1000 topics. These topics were graciously provided by Ricky Wong.}}{15}}
\newlabel{table:example_lda_topics}{{2.2}{15}}
\@writefile{toc}{\contentsline {subsubsection}{Inference}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Ideal point models and matrix factorization}{15}}
\citation{wang:2011,salakhutdinov:2008a,poole:1985,poole:1991,clinton:2004}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Probabilistic matrix factorization. We observe interactions $V_{ud}$ between users represented by $X_u$ and items represented by $A_d, B_d$.}}{16}}
\newlabel{figure:irt_gm}{{2.3}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Hidden Markov Models and Kalman Filters}{16}}
\citation{bishop:2006}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A hidden Markov model. Observations $Y_1, \ldots  , Y_T$ are observed at discrete times $t=1, \ldots  , T$, and are conditionally independent given the hidden states $X_1, \ldots  , X_T$.}}{17}}
\newlabel{figure:hmm_gm}{{2.4}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Posterior inference and model evaluation}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}MAP estimation}{17}}
\citation{jordan:2003,jordan:1999}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}MCMC}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Variational inference}{18}}
\newlabel{section:variational_inference}{{2.3.3}{18}}
\newlabel{equation:variational_objective}{{2.10}{18}}
\citation{jordan:1999,gerrish:2011}
\citation{bishop:2006}
\citation{blei:2003}
\citation{bickel:2007,braun:2007}
\newlabel{figure:variational_inference}{{2.3.3}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Illustration of variational inference. Practitioners define a variational family (illustrated in yellow) and find the member of that familt $q_{\mathaccentV {hat}05E\eta }(x)$ which is closest (by KL divergence) to the true posterior.}}{19}}
\newlabel{equation:traditional_variational_objective}{{2.11}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Model evaluation}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Likelihood of training data $Y_1, \ldots  , Y_{N_{\unhbox \voidb@x \hbox {obs}}}$}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Likelihood of heldout observations $Y_1, \ldots  , Y_{N_{\unhbox \voidb@x \hbox {heldout}}}$}{20}}
\@writefile{toc}{\contentsline {subsubsection}{Relationship with external data}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Using these tools to understand influence and decisionmaking}{21}}
\citation{garfield:2002}
\citation{brin:1998}
\citation{osareh:1996}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}A model of influence in text documents}{22}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{blei:2006}
\citation{tang:2009}
\citation{lokker:2008}
\citation{tang:2009,lokker:2008}
\citation{lokker:2008}
\citation{ibanez:2009}
\citation{lokker:2008}
\citation{lokker:2008}
\citation{nallapati:2008,chang:2009,dietz:2007,Cohn01themissing}
\citation{mcnee:2002,ibanez:2009}
\citation{qazvinian:2008}
\citation{mann:2006}
\citation{borner:2003}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}The Document Influence Model}{24}}
\newlabel{section:model}{{3.1}{24}}
\citation{blei:2006}
\citation{blei:2003,deerwester:1990,hofmann:1999}
\citation{blei:2006}
\newlabel{fig:doc_influence_model}{{3.1}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  The Dynamic Topic Model (a) and the the Document Influence Model (b).}}{25}}
\newlabel{fig:gm}{{3.1}{25}}
\@writefile{toc}{\contentsline {paragraph}{Drifting Topics.}{25}}
\newlabel{eq:softmax}{{3.1}{25}}
\newlabel{eq:logistic-normal}{{3.2}{25}}
\@writefile{toc}{\contentsline {paragraph}{Documents generated at time $t$.}{25}}
\newlabel{eq:logistic-normal-influence}{{3.3}{26}}
\citation{blei:2003}
\citation{porter:2005}
\citation{leskovec:2009}
\citation{shaparenko:2007}
\citation{shaparenko:2007}
\@writefile{toc}{\contentsline {paragraph}{Multiple topics.}{27}}
\newlabel{eq:logistic-normal-influence-topics}{{3.4}{27}}
\citation{blei:2003}
\citation{blei:2006}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Inference and parameter estimation}{28}}
\newlabel{section:inference}{{3.2}{28}}
\citation{blei:2006}
\citation{blei:2006}
\citation{blei:2006}
\@writefile{toc}{\contentsline {paragraph}{Topic trajectories.}{30}}
\@writefile{toc}{\contentsline {paragraph}{Influence values.}{30}}
\citation{blei:2003}
\@writefile{toc}{\contentsline {paragraph}{Topic proportions and topic assignments.}{31}}
\citation{bird:2008}
\citation{Radev:2009}
\citation{Thompson:2009}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Empirical study}{32}}
\newlabel{sec:data}{{3.3}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Spearman rank correlation between citation counts and posterior influence score, controlling for date (top) and fraction of citations explained by posterior influence (bottom).}}{33}}
\newlabel{fig:results}{{3.2}{33}}
\newlabel{sec:results}{{3.3}{33}}
\newlabel{eq:score}{{3.7}{33}}
\@writefile{toc}{\contentsline {paragraph}{Heuristic model.}{34}}
\@writefile{toc}{\contentsline {paragraph}{Shuffled corpus}{34}}
\citation{brown:1993}
\citation{brown:1993}
\citation{marcus:1993}
\citation{marcus:1993}
\citation{brown:1993}
\citation{toole:1984}
\citation{brown:1993}
\citation{toole:1984}
\citation{Nature.success:1969}
\@writefile{toc}{\contentsline {paragraph}{IBM Model 3}{35}}
\@writefile{toc}{\contentsline {paragraph}{The Penn Treebank}{35}}
\@writefile{toc}{\contentsline {paragraph}{Success in 1972}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Most active words appearing in \cite  {brown:1993} (left) which have changed the most in a topic about translation. On right are words appearing in \cite  {toole:1984} in a topic about DNA and genetics. Terms are sorted by increase over 10 years.}}{36}}
\newlabel{fig:words}{{3.3}{36}}
\citation{NSF.website:2010}
\citation{toole:1984}
\citation{}
\citation{beim:2011}
\@writefile{toc}{\contentsline {paragraph}{Genetics in \emph  {Nature}}{37}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}New York Appellate Courts.}{37}}
\@writefile{toc}{\contentsline {paragraph}{The Cardozo Topic.}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}A parallel implementation of the model}{38}}
\newlabel{section:influence_parallel_inference}{{3.5}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}A parallel implementation of the model}{40}}
\newlabel{section:influence_parallel_inference}{{3.6}{40}}
\citation{brown:1993}
\citation{toole:1984}
\citation{marcus:1993}
\citation{leskovec:2009}
\citation{alvarez:2009}
\citation{shaparenko:2007}
\citation{dietz:2007}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Conclusions and future work}{43}}
\citation{martin:2002,jackman:2001}
\citation{hoff:2002,chang:2009}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}A time-series model of foreign affairs: sentiment between nation-states}{44}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A time-series model of countries' interactions. Pseudo-observations of ``zero'' are added for regularization. Amazon Mechanical Turk labels are used to fit $\beta $, which is used to infer unobserved sentiments.}}{44}}
\newlabel{figure:gm}{{4.1}{44}}
\citation{kogan:2009}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Foreign relations and their influence on news reporting}{45}}
\newlabel{section:model}{{4.1}{45}}
\@writefile{toc}{\contentsline {paragraph}{A temporal model of interaction.}{45}}
\newlabel{equation:sentiment}{{4.2}{45}}
\newlabel{section:text_regression}{{4.1}{45}}
\citation{gartzke:1998}
\citation{hoff:2002,sarkar:2005}
\citation{chang:2009}
\citation{kogan:2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Related work}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Inference}{46}}
\@writefile{toc}{\contentsline {paragraph}{M Step.}{46}}
\@writefile{toc}{\contentsline {paragraph}{E Step.}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Estimation of sentiment $s$}{47}}
\newlabel{section:sentiment_models}{{4.3}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}A latent-space model of foreign relations}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Posterior inference}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Supervised sentiment analysis}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Unsupervised sentiment analysis}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Empirical studies: comparisons with ground truth}{47}}
\newlabel{section:experiments}{{4.5}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Datasets and tokenization}{47}}
\newlabel{figure:mechanical_turk_sample}{{4.5.2}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A screenshot of a Mechanical Turk labeling task. Sometimes relationships may be complicated; both raters gave this example a score of ``slightly positive''.}}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Mechanical Turk Labels}{48}}
\newlabel{section:mturk}{{4.5.2}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Results}{48}}
\@writefile{toc}{\contentsline {paragraph}{Heldout sentiment}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces  (a) Example positions of selected countries in the latent space of national sentiment, in 1987. Sentiment is given by the inner product between two vectors. (b) Example positions of selected countries in 2007. (c) Mutual sentiment $s = \mathaccentV {bar}016x_{c, \cdot }^T \mathaccentV {bar}016x_{\unhbox \voidb@x \hbox {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip us}, \cdot }$ with the United States over time. The two Iraq wars and September 11th, 2001 are marked. }}{49}}
\newlabel{figure:figures}{{4.3}{49}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Average error predicting sentiment $s_d$ between heldout nation pairs.}}{49}}
\newlabel{figure:sse_test}{{4.1}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Comparisons with Ground Truth}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Relation to ideal points inferred from the UN General Assembly}{50}}
\citation{govtrack:2009}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Models of Spatial Voting and text}{51}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{The role of text in lawmaking bodies.}{51}}
\citation{poole:1985,poole:1991,jackman:2001,martin:2002,clinton:2004}
\citation{poole:1991,clinton:2004}
\citation{clinton:2004}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Example one-dimensional ideal points from the 111th House of Representatives. Ideal points represent lawmakers' voting preferences. Democrats are blue and Republicans are red.}}{52}}
\newlabel{figure:example_ideal_points}{{5.1}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}The Ideal Point Model}{52}}
\newlabel{sec:model}{{5.1}{52}}
\newlabel{eq:trad_ipm}{{5.1}{52}}
\citation{clinton:2004}
\citation{lord:1980}
\citation{blei:2008}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}A model for predicting votes with the text of new bills}{53}}
\citation{Kogan:2009}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces The ideal point topic model. Priors over the multinomials $\theta _d$ and $\beta $ are both symmetric Dirichlet distributions.}}{54}}
\newlabel{fig:legis_gm}{{5.2}{54}}
\citation{blei:2008}
\citation{blei:2003}
\newlabel{eq:posterior}{{5.2}{55}}
\citation{jackman:2001,clinton:2004,martin:2002}
\citation{clinton:2004}
\citation{enelow:1984}
\citation{poole:1985,heckman:1996}
\citation{jackman:2001,martin:2002,clinton:2004}
\citation{martin:2002,wang:2010}
\citation{jackman:2001,heckman:1996}
\citation{johnson:1999ch6}
\citation{quinn:2006}
\citation{thomas:2006}
\citation{pang:2008}
\citation{Salakhutdinov:2008a}
\citation{agarwal:2010}
\citation{wang:2010}
\citation{johnson:1999ch6,jackman:2001,martin:2002,clinton:2004}
\citation{jordan:1999}
\citation{blei:2003}
\newlabel{sec:inference}{{5.2}{57}}
\newlabel{eq:var_post}{{5.3}{57}}
\citation{treetagger}
\newlabel{sec:experiments}{{5.2}{58}}
\newlabel{fig:log_likelihood}{{5.2}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Vote log likelihood on heldout votes. Models are shown by color for different regularizations (x axis), for Congresses 106 to 111. For LARS and L2, the regularization is the complexity parameter; for the ITPM, the regularization is the the number of topics. The \emph  {yea} baseline is the horizontal black line. LARS is below the fold for 106-107. The ideal point topic model performs with less variance across its regularization parameter. }}{59}}
\citation{herszenhorn:2010}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Topics can be visualized in the same latent political space as legislators and bills. This plot shows selected topics by coefficients $\bm  {\mathaccentV {hat}05E}\eta $, for a 64-topic model ($\bm  {\mathaccentV {hat}05E}\eta $s are normalized by mean and variance). Two topics (\emph  {people, month, recognize, ...} and \emph  {clause, motion, chair, ...}) with difficulty 4.68 and discrimination 7.4 (respectively) are not shown.}}{60}}
\newlabel{fig:topics}{{5.4}{60}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}A study of lawmakers' voting on specific issues}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces In a traditional ideal point model, lawmakers' ideal points are static (top line of each figure). In the issue-adjusted ideal point model, lawmakers' ideal points change when they vote on certain issues, such as \emph  {Taxation} (top) and \emph  {Health} (bottom).}}{63}}
\newlabel{fig:moving_ideal_points}{{5.5}{63}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}A model of exceptional voting patterns}{63}}
\newlabel{section:exceptional_model}{{5.3.1}{63}}
\@writefile{toc}{\contentsline {subsubsection}{Limitations of ideal point models.}{63}}
\citation{ramage:2009}
\citation{ramage:2009}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Left: the issue-adjusted ideal point model, which models votes $v_{ud}$ from lawmakers and legislative items. Classic item response theory models votes $v$ using $x_u$ and $a_d, b_d$. For our work, documents' issue vectors $\bm  {\theta }$ were estimated fit with a topic model (left of dashed line) using bills' words $w$ and labeled topics $\beta $. Expected issue vectors $\mathbb  {E}_q\left [\bm  {\theta }| \bm  {w}\right ]$ are then treated as constants in the issue model (right of dashed line). Right: Top words from topics fit using labeled LDA \cite  {ramage:2009}. }}{64}}
\newlabel{figure:legis_gm}{{5.6}{64}}
\newlabel{table:example_topics}{{5.6}{64}}
\@writefile{toc}{\contentsline {subsubsection}{Issue-adjusted ideal points.}{64}}
\citation{ramage:2009}
\citation{blei:2003}
\citation{ramage:2009}
\citation{crs:2011}
\newlabel{equation:exploratory_ipm_old}{{5.4}{65}}
\@writefile{toc}{\contentsline {subsubsection}{Using Labeled LDA to associate bills with issues.}{65}}
\newlabel{section:lda}{{5.3.1}{65}}
\citation{clinton:2004,martin:2002,poole:1985}
\citation{enelow:1984}
\citation{albert:1992}
\citation{heckman:1996}
\citation{jackman:2001}
\citation{gerrish:2011}
\citation{wang:2010}
\citation{wang:2011}
\citation{agarwal:2010}
\citation{agarwal:2010}
\citation{wang:2011}
\@writefile{toc}{\contentsline {subsubsection}{Related Work.}{66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Inference for the adjusted ideal point model}{66}}
\newlabel{section:inference}{{5.3.2}{66}}
\citation{johnson:1999ch6,jackman:2001,martin:2002,clinton:2004}
\citation{jordan:1999}
\citation{gerrish:2011}
\citation{robbins:1951,bottou:2004}
\newlabel{equation:variational_posterior}{{5.5}{67}}
\newlabel{equation:approx_elbo_gradient}{{5.6}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}An empirical analysis of voting patterns}{67}}
\newlabel{section:empirical_analysis}{{5.3.3}{67}}
\citation{govtrack:2009}
\citation{gerrish:2011}
\@writefile{toc}{\contentsline {subsubsection}{Data and Experiment Setup}{68}}
\newlabel{section:corpus_details}{{5.3.3}{68}}
\@writefile{toc}{\contentsline {subsubsection}{Comparison of classic and exploratory ideal points}{68}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation and significance}{68}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Average log-likelihood of heldout votes using six-fold cross validation. These results cover Congresses 106 to 111 (1999-2010) with regularization $\lambda =1$. The issue-adjusted model yields higher heldout log-likelihood for all congresses in both chambers than a standard ideal point model. Perm. Issue illustrates the issue model fit when bills' issue labels were randomly permuted. \emph  {Perm. Issue} is results for the issue model fit using permuted document labels.}}{69}}
\newlabel{table:performance_comparison}{{5.1}{69}}
\newlabel{section:performance}{{5.3.3}{69}}
\citation{fenno:1965,cox:1993,cox:2002,cox:2005}
\@writefile{toc}{\contentsline {subsubsection}{Checking issue affinity}{70}}
\@writefile{toc}{\contentsline {subsubsection}{Issues improved by issue adjustment}{70}}
\newlabel{equation:likelihood_improvement}{{5.8}{70}}
\@writefile{toc}{\contentsline {subsubsection}{Analyzing lawmakers}{70}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Ideal points $x_u$ and issue-adjusted ideal points $x_u + z_{uk}$ from the 111th House. Votes about Finance were better fit with this model. Republicans (red) saw more adjustment than Democrats (blue). }}{71}}
\newlabel{figure:issue_improvements_ideals}{{5.7}{71}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Significant issue adjustments for exceptional senators in Congress 111. Statistically significant issue adjustments are shown with each $\times $.}}{72}}
\newlabel{figure:significant_offsets}{{5.8}{72}}
\@writefile{toc}{\contentsline {subsubsection}{Issue model misfit}{73}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}The influence of Judges' opinions in higher courts}{74}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction: Decisions and influence in the higher courts}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Influence and the progress of ideas in the higher courts}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}The Cardozo Topic}{74}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Influence of Judges' dissenting opinions}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}A model of influence among opinions}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}A posterior-predictive checks among dissenting opinions}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}Do lower court opinions influence higher courts?}{74}}
\citation{bishop:2006}
\citation{jordan:1999}
\citation{winn:2004}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Appendix}{75}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}A.1. Optimizing the variational bound stochastically}{75}}
\citation{carbonetto:2009,graves:2011}
\citation{carbonetto:2009}
\citation{graves:2011}
\citation{carbonetto:2009}
\citation{jordan:2003,jordan:1999}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Stochastic optimization of the variational objective}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Variational inference}{76}}
\newlabel{equation:variational_objective}{{7.1}{76}}
\citation{jaakkola:2000,jordan:1999,bickel:2007,braun:2007}
\citation{graves:2011,wei:1990,carbonetto:2009}
\citation{robbins:1951}
\newlabel{equation:traditional_variational_objective}{{7.2}{77}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Stochastic optimization of the variational objective}{77}}
\newlabel{equation:gradient-as-expectation}{{7.4}{77}}
\citation{carbonetto:2009,graves:2011}
\citation{wei:1990}
\citation{carbonetto:2009}
\citation{bottou:2004,robbins:1951}
\newlabel{equation:svo_gradient}{{7.5}{78}}
\newlabel{equation:svo_update}{{7.6}{78}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces A non-conjugate posterior density (dashed) and a Gaussian variational approximation (solid).}}{79}}
\newlabel{figure:univariate_comparison_approximation}{{7.1}{79}}
\@writefile{toc}{\contentsline {subsubsection}{Example: Gaussian variational marginal}{79}}
\newlabel{section:gaussian}{{7.2.2}{79}}
\newlabel{equation:gaussian_mean_gradient}{{7.7}{79}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces A comparison of our algorithm using first-order vs. second-order updates (top vs. bottom); and estimating gradients using \emph  {iid} samples from $q$ vs. quasi-Monte Carlo samples from $q$ (left vs. right).}}{80}}
\newlabel{figure:comparision_order_sampling}{{7.2}{80}}
\newlabel{equation:gaussian_mean}{{7.9}{80}}
\newlabel{equation:gaussian_variance_gradient}{{7.10}{80}}
\@writefile{toc}{\contentsline {paragraph}{Testing convergence.}{80}}
\citation{tang:1993,owen:1998,niederreiter:1992}
\citation{wei:1990}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Improving performance}{81}}
\@writefile{toc}{\contentsline {subsubsection}{Minibatch sampling}{81}}
\newlabel{section:minibatch_sampling}{{7.2.3}{81}}
\citation{robbins:1951,bottou:2004}
\citation{carbonetto:2009}
\citation{wei:1990}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Second-order SVO. We begin with a variational distribution $q_{\theta _0}(x)$ and joint likelihood $p(x, y)$. }}{82}}
\newlabel{figure:second_order_algorithm}{{1}{82}}
\@writefile{toc}{\contentsline {subsubsection}{Second-order updates}{82}}
\newlabel{section:second_order_updates}{{7.2.3}{82}}
\citation{bottou:2004}
\citation{hoffman:2010}
\newlabel{equation:empirical_second_order_estimates}{{7.12}{83}}
\newlabel{equation:second_order_updates}{{7.13}{83}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.4}Multivariate distributions}{83}}
\citation{carbonetto:2009}
\citation{carbonetto:2009}
\citation{wei:1990}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.5}Related work}{84}}
\@writefile{toc}{\contentsline {paragraph}{Stochastic optimization.}{84}}
\@writefile{toc}{\contentsline {paragraph}{Stochastic sampling with variational inference.}{84}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Empirical study}{84}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Experimental results comparing SVO and MCMC estimates. We show lawmaker posteriors in the probit IRT model (left) and observation means from a change point model (right). In each table we illustrate runtime, log-likelihood (LL) or mean-squared error (MSE) on heldout observations. We also estimate MSE against the ``True'' posterior means, estimated using long Gibbs runs (500K and 50K samples for left and right respectively). }}{85}}
\newlabel{figure:changepoint_results}{{7.3}{85}}
\newlabel{figure:irt_results}{{7.3}{85}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Univariate examples}{85}}
\newlabel{section:univariate_example}{{7.3.1}{85}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces figure}}{86}}
\newlabel{figure:univariate_comparison}{{7.4}{86}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Well-log data (grey) fit with a variational switching Kalman filter. The inferred means $\mathaccentV {bar}016\mu _{t}$ of the filter are shown in black. Each timestamp also has an associated variational change point $\mathaccentV {bar}016c_t$ which indicates the probability that the filter is making a large transition. Transitions at change points with mean $\mathaccentV {bar}016c_t > \frac  {1}{2}$ are marked in red.}}{86}}
\newlabel{figure:switching_kalman_filter}{{7.5}{86}}
\citation{poole:1991,martin:2002,albert:1992}
\citation{clinton:2004}
\citation{albert:1992}
\citation{gerrish:2011}
\citation{armagan:2011}
\citation{murphy:1998}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Probit regression and ideal points}{87}}
\newlabel{section:probit_regression}{{7.3.2}{87}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Switching Kalman filter}{87}}
\newlabel{section:switching_kalman_filter}{{7.3.3}{87}}
\citation{ghahramani:1996,murphy:1998}
\citation{ghahramani:1996}
\citation{adams:2007}
\citation{ruanaidh:1996,adams:2007}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.4}Alternative variational distributions: Laplace variational posterior}{89}}
\newlabel{section:complicated_posteriors}{{7.3.4}{89}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Discussion}{89}}
\bibstyle{unsrt}
\bibdata{bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Appendix}{90}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Processing text data}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}Stemming and parsing}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.2}Current libraries and software}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Model estimation basics}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Multimodal distributions and identification}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Setup of a typical program}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}The role of traditional dimensionality reduction in evaluating underconstrained distributions}{90}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Stochastic optimization}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Round-robin updates}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Bounded step size}{90}}
\bibcite{lazer:2009}{{1}{}{{}}{{}}}
\bibcite{googleblog:2008}{{2}{}{{}}{{}}}
\bibcite{changrtl:2009}{{3}{}{{}}{{}}}
\bibcite{pearl:1985}{{4}{}{{}}{{}}}
\bibcite{box:1980}{{5}{}{{}}{{}}}
\bibcite{gelman:1996}{{6}{}{{}}{{}}}
\bibcite{tufte:2001}{{7}{}{{}}{{}}}
\bibcite{wilkinson:2005}{{8}{}{{}}{{}}}
\bibcite{chaney:2012}{{9}{}{{}}{{}}}
\bibcite{bishop:2006}{{10}{}{{}}{{}}}
\bibcite{grimmer:submitted}{{11}{}{{}}{{}}}
\bibcite{blei:2003}{{12}{}{{}}{{}}}
\bibcite{salakhutdinov:2008a}{{13}{}{{}}{{}}}
\bibcite{clinton:2004}{{14}{}{{}}{{}}}
\bibcite{martin:2002}{{15}{}{{}}{{}}}
\bibcite{poole:1991}{{16}{}{{}}{{}}}
\bibcite{enelow:1984}{{17}{}{{}}{{}}}
\bibcite{albert:1992}{{18}{}{{}}{{}}}
\bibcite{wang:2011}{{19}{}{{}}{{}}}
\bibcite{poole:1985}{{20}{}{{}}{{}}}
\bibcite{jordan:2003}{{21}{}{{}}{{}}}
\bibcite{jordan:1999}{{22}{}{{}}{{}}}
\bibcite{bickel:2007}{{23}{}{{}}{{}}}
\bibcite{braun:2007}{{24}{}{{}}{{}}}
\bibcite{garfield:2002}{{25}{}{{}}{{}}}
\bibcite{brin:1998}{{26}{}{{}}{{}}}
\bibcite{osareh:1996}{{27}{}{{}}{{}}}
\bibcite{blei:2006}{{28}{}{{}}{{}}}
\bibcite{tang:2009}{{29}{}{{}}{{}}}
\bibcite{lokker:2008}{{30}{}{{}}{{}}}
\bibcite{ibanez:2009}{{31}{}{{}}{{}}}
\bibcite{nallapati:2008}{{32}{}{{}}{{}}}
\bibcite{chang:2009}{{33}{}{{}}{{}}}
\bibcite{dietz:2007}{{34}{}{{}}{{}}}
\bibcite{Cohn01themissing}{{35}{}{{}}{{}}}
\bibcite{mcnee:2002}{{36}{}{{}}{{}}}
\bibcite{qazvinian:2008}{{37}{}{{}}{{}}}
\bibcite{mann:2006}{{38}{}{{}}{{}}}
\bibcite{borner:2003}{{39}{}{{}}{{}}}
\bibcite{deerwester:1990}{{40}{}{{}}{{}}}
\bibcite{hofmann:1999}{{41}{}{{}}{{}}}
\bibcite{porter:2005}{{42}{}{{}}{{}}}
\bibcite{leskovec:2009}{{43}{}{{}}{{}}}
\bibcite{shaparenko:2007}{{44}{}{{}}{{}}}
\bibcite{bird:2008}{{45}{}{{}}{{}}}
\bibcite{Radev:2009}{{46}{}{{}}{{}}}
\bibcite{Thompson:2009}{{47}{}{{}}{{}}}
\bibcite{brown:1993}{{48}{}{{}}{{}}}
\bibcite{marcus:1993}{{49}{}{{}}{{}}}
\bibcite{toole:1984}{{50}{}{{}}{{}}}
\bibcite{Nature.success:1969}{{51}{}{{}}{{}}}
\bibcite{NSF.website:2010}{{52}{}{{}}{{}}}
\bibcite{beim:2011}{{53}{}{{}}{{}}}
\bibcite{alvarez:2009}{{54}{}{{}}{{}}}
\bibcite{winn:2004}{{55}{}{{}}{{}}}
\bibcite{carbonetto:2009}{{56}{}{{}}{{}}}
\bibcite{graves:2011}{{57}{}{{}}{{}}}
\bibcite{jaakkola:2000}{{58}{}{{}}{{}}}
\bibcite{wei:1990}{{59}{}{{}}{{}}}
\bibcite{robbins:1951}{{60}{}{{}}{{}}}
\bibcite{bottou:2004}{{61}{}{{}}{{}}}
\bibcite{tang:1993}{{62}{}{{}}{{}}}
\bibcite{owen:1998}{{63}{}{{}}{{}}}
\bibcite{niederreiter:1992}{{64}{}{{}}{{}}}
\bibcite{hoffman:2010}{{65}{}{{}}{{}}}
\bibcite{gerrish:2011}{{66}{}{{}}{{}}}
\bibcite{armagan:2011}{{67}{}{{}}{{}}}
\bibcite{murphy:1998}{{68}{}{{}}{{}}}
\bibcite{ghahramani:1996}{{69}{}{{}}{{}}}
\bibcite{adams:2007}{{70}{}{{}}{{}}}
\bibcite{ruanaidh:1996}{{71}{}{{}}{{}}}
\bibcite{jackman:2001}{{72}{}{{}}{{}}}
\bibcite{hoff:2002}{{73}{}{{}}{{}}}
\bibcite{kogan:2009}{{74}{}{{}}{{}}}
\bibcite{gartzke:1998}{{75}{}{{}}{{}}}
\bibcite{sarkar:2005}{{76}{}{{}}{{}}}
\bibcite{lord:1980}{{77}{}{{}}{{}}}
\bibcite{blei:2008}{{78}{}{{}}{{}}}
\bibcite{heckman:1996}{{79}{}{{}}{{}}}
\bibcite{wang:2010}{{80}{}{{}}{{}}}
\bibcite{johnson:1999ch6}{{81}{}{{}}{{}}}
\bibcite{quinn:2006}{{82}{}{{}}{{}}}
\bibcite{thomas:2006}{{83}{}{{}}{{}}}
\bibcite{pang:2008}{{84}{}{{}}{{}}}
\bibcite{agarwal:2010}{{85}{}{{}}{{}}}
\bibcite{treetagger}{{86}{}{{}}{{}}}
\bibcite{herszenhorn:2010}{{87}{}{{}}{{}}}
\bibcite{ramage:2009}{{88}{}{{}}{{}}}
\bibcite{crs:2011}{{89}{}{{}}{{}}}
\bibcite{govtrack:2009}{{90}{}{{}}{{}}}
\bibcite{fenno:1965}{{91}{}{{}}{{}}}
\bibcite{cox:1993}{{92}{}{{}}{{}}}
\bibcite{cox:2002}{{93}{}{{}}{{}}}
\bibcite{cox:2005}{{94}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
